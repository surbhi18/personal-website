One of the greatest challenges in AI today is making systems trustworthy. Surbhi, can you speak to some of your work in this area? 
AI models today are sort of like these gigantic blackboxes and we often know very little about them. The way in which they make predictions can be super complex, and makes trusting them very challenging. My research hopes to open this blackbox and understand what's going on under the hood. In particular, I use mathematical tools to understand what these models can and cannot do, how they learn useful functionalities, when and how do they fail. For example, reasoning is a powerful skill that LLMs seem to be getting better at. But how are they reasoning? In joint work with folks at CMU, and MSR, we showed that these models reason in a very different way than what humans would do, and this led us to identify potential failure modes, and causes for hallucination.


Why aren’t algorithms always designed with ethics in mind? What does it take to design ethical algorithms? 
This is a very good question, and often not given enough importance. Generally algorithms are designed with certain objectives in mind, maybe that may be improving the accuracy of your model, or speed, or maybe usability. Such performance measures often ignore ethical considerations of how is it that the model is increasing its accuracy, who are the predictions hurting more. Furthermore, incentives are often not aligned to prioritize ethical considerations, and even if they were, it is not often easy to formalize and evaluate these. 

To design ethical algorithms, it is first of all important for the engineer/researcher to have awareness and knowledge of the repurcussions of their design decisions. Proper documentation of data used (datasheets for datasets), rigorous evaluation, and reviews of AI models, diversity of opinion on the team, incentive alignment towards prioritizing these objectives


Surbhi, what makes it so hard for AI systems to be explainable? Why can’t we look inside, say, a large language model, and understand what it’s doing, the way we could with a car? (Mathematical foundations)

The car is built by combining various different components that each serve a particular role (which was both well-understood and well-tested). We very well understand the physics, mechanichs etc. This has of course taken years of work from scientists and engineers to make it into a functioning system that we can drive around. 

However unlike cars, the mechanism of designing an AI system in today's day is rather different. Instead of designing components and combining them together in a known way, we instead fix some architecture or connections between different components, and then use large amounts of data to update these connections using an optimization algorithm that optimizes some objective. It is not even clear what part of the network has learned what functionality, and whether it has even learned it perfectly. Beyond this, the scale of the number of parameters is huge, a visualization that is helpful is to think of one parameter as a 1cm square of area, and the nymber of parameters as 2500 soccer fields! Trying to interpret such a large network can be very challenging. It is also not clear if the functionality is so disentangled as in a car, is there actually a component that operates as the engine, and brakes. 