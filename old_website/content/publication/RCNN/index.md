---
title: "Recurrent Convolutional Neural Networks Learn Succinct Learning Algorithms"
publication_types:
  - "1"
authors:
  - admin
  - Sham Kakade
  - Adam Tauman Kalai
  - Cyril Zhang
author_notes:
  - Alphabetical Ordering
  - Alphabetical Ordering
  - Alphabetical Ordering
  - Alphabetical Ordering
abstract: "Neural Networks (NNs) struggle to efficiently learn certain problems, such as parity problems, even when there are simple learning algorithms for those problems. Can NNs discover learning algorithms on their own? We exhibit a NN architecture that, in polynomial time, learns as well as any efficient learning algorithm describable by a constant-sized learning algorithm. For example, on parity problems, the NN learns as well as row reduction, an efficient algorithm that can be succinctly described. Our architecture combines both recurrent weight-sharing between layers and convolutional weight-sharing to reduce the number of parameters down to a constant, even though the network itself may have trillions of nodes. While in practice the constants in our analysis are too large to be directly meaningful, our work suggests that the synergy of Recurrent and Convolutional NNs (RCNNs) may be more powerful than either alone."
draft: false
featured: true
url_pdf: https://arxiv.org/abs/2209.00735.pdf
publication_short: NeurIPS 2022
publication: Neural Information Processing Systems (NeurIPS) 2022
date: "2022-09-01"
---
